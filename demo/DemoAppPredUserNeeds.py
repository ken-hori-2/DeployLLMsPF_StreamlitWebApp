from openai import OpenAI
import streamlit as st
import streamlit_antd_components as sac
import os
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

def chat():
    st.title("ChatGPT by Streamlit (Prediction User Needs)")
    st.write("**it is a simple chatbot made only with OpenAI and Streamlit. You can set system prompts, model, and temperature as options.**")
    client = OpenAI(api_key=st.session_state.openai_api_key)

    if "openai_model" not in st.session_state:
        # st.session_state["openai_model"] = "gpt-4-turbo-preview"
        # st.session_state["openai_model"] = "gpt-4o"
        st.session_state["openai_model"] = "gpt-3.5-turbo"

    if "Clear" not in st.session_state:
        st.session_state.Clear = False
   
    if "temperature" not in st.session_state:
        # st.session_state.temperature = 0.7
        st.session_state.temperature = 0.0
    

    # 2024/05/23
    pre_template = """
               ã‚ãªãŸã¯äººé–“ã¨è©±ã™ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«ç­”ãˆã¦ãã ã•ã„ã€‚

               
               """
    # 2024/05/23
    
    if "system_prompt" not in st.session_state:
        # st.session_state.system_prompt = "You are an AI chatbot having a conversation with a human."
        st.session_state.system_prompt = pre_template

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role":"system", 
                "content":st.session_state.system_prompt
            }
        ]

    with st.expander("Options"):
        # st.selectbox("Model", ("gpt-4-turbo-preview", "gpt-3.5-turbo"), help="Choose the AI model to use. 'gpt-4-turbo-preview' is the latest model with more advanced capabilities, while 'gpt-3.5-turbo' is an older but still powerful version.", key="openai_model")
        st.selectbox(
            "Model", 
            ("gpt-4o", "gpt-3.5-turbo"), 
            help="Choose the AI model to use. 'gpt-4o' is the latest model with more advanced capabilities, while 'gpt-3.5-turbo' is an older but still powerful version.", 
            key="openai_model"
            )
        st.text_area(
            "System Prompt", 
            #  value="You are an AI chatbot having a conversation with a human.", 
            value=pre_template, 
            help="Can only be set at the time of the first message sent.Set the initial prompt for the AI system which sets the context of the conversation. This can influence how the AI responds.", 
            key="system_prompt"
            )
        st.slider(
            "Temperature", 
            min_value=0.0, 
            max_value=1.0, 
            value=0.7, 
            help="Adjust the creativity of the AI's responses. A lower temperature means more deterministic and predictable responses, while a higher temperature results in more varied and sometimes more creative responses.", 
            key="temperature"
            )

    for message in st.session_state.messages:
        if not message["role"]=="system":
            if message["role"]=="user":
                with st.chat_message(message["role"], avatar = "ğŸ˜Š"):
                    st.markdown(message["content"])
            elif message["role"]=="assistant":
                with st.chat_message(message["role"], avatar = "ğŸ¤–"):
                    st.markdown(message["content"])

    if prompt := st.chat_input("What is up?"):
        if st.session_state.openai_api_key == "":
            sac.alert(
                label='warning', 
                description='Please add your OpenAI API key to continue.', 
                color='red', 
                banner=[False, True], 
                icon=True, 
                size='lg'
            )
            st.stop()
        

        
        
        llm = ChatOpenAI(temperature=0, streaming=True, model="gpt-3.5-turbo",openai_api_key=st.session_state.openai_api_key)
        prompt = PromptTemplate(
            input_variables=["UserAction"],
            # template="{job}ã«ä¸€ç•ªã‚ªã‚¹ã‚¹ãƒ¡ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã¯ä½•?"
            # ãã®å¾Œã€ç¾åœ¨ãŒ{time}ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•çŠ¶æ…‹ãŒ{UserAction}ã®å ´åˆã©ã®æ©Ÿèƒ½ã‚’ææ¡ˆã™ã‚‹ã‹æ•™ãˆã¦ãã ã•ã„ã€‚
            # template="""
            #     ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®1æ—¥ã®è¡Œå‹•ã¨ä½¿ã£ãŸæ©Ÿèƒ½ã§ã™ã€‚
            #     ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å‚¾å‘ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚(ä¾‹: 1. æ™‚é–“å¸¯ã”ã¨ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³: , 2. è¡Œå‹•çŠ¶æ…‹: )
            #     ãã®å¾Œã€ç¾åœ¨ãŒ11æ™‚30åˆ†ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•çŠ¶æ…‹ãŒ{UserAction}ã®å ´åˆã©ã®æ©Ÿèƒ½ã‚’ææ¡ˆã™ã‚‹ã‹æ•™ãˆã¦ãã ã•ã„ã€‚
            #     ãã®éš›ã€å„æ©Ÿèƒ½ã®ææ¡ˆã™ã‚‹ç¢ºç‡ã¨æœ€çµ‚çš„ãªææ¡ˆ(Final Answer:)ã‚‚æ•™ãˆã¦ãã ã•ã„ã€‚

            #     7:30, STABLE, å¤©æ°—æƒ…å ±
            #     8:00, WALKING, çµŒè·¯æ¤œç´¢
            #     8:30, STABLE, æ¥½æ›²å†ç”Ÿ
            #     9:00, STABLE, ä¼šè­°æƒ…å ±
            #     11:00, WALKING, ä¼šè­°æƒ…å ±
            #     12:00, STABLE, ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æ¤œç´¢
            #     12:30, STABLE, æ¥½æ›²å†ç”Ÿ
            #     15:00, WALKING, ä¼šè­°æƒ…å ±
            #     17:30, STABLE, çµŒè·¯æ¤œç´¢
            #     18:00, STABLE, æ¥½æ›²å†ç”Ÿ
            #     19:00, RUNNING, æ¥½æ›²å†ç”Ÿ
            #     19:30, RUNNING, æ¥½æ›²å†ç”Ÿ

            #     ã‚ãªãŸãŒææ¡ˆã§ãã‚‹æ©Ÿèƒ½ã¯
            #     "ä¼šè­°æƒ…å ±", "æ¥½æ›²å†ç”Ÿ", "çµŒè·¯æ¤œç´¢", "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±æ¤œç´¢", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æ¤œç´¢", "ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±", "å¤©æ°—æƒ…å ±"
            #     ã§ã™ã€‚
            #     """
            template="""
                ç¾åœ¨ãŒ11æ™‚30åˆ†ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•çŠ¶æ…‹ãŒ{UserAction}ã®å ´åˆã©ã®æ©Ÿèƒ½ã‚’ææ¡ˆã™ã‚‹ã‹æ•™ãˆã¦ãã ã•ã„ã€‚
                ãã®éš›ã€å„æ©Ÿèƒ½ã®ææ¡ˆã™ã‚‹ç¢ºç‡ã¨æœ€çµ‚çš„ãªææ¡ˆ(Final Answer:)ã‚‚æ•™ãˆã¦ãã ã•ã„ã€‚

                ã‚ãªãŸãŒææ¡ˆã§ãã‚‹æ©Ÿèƒ½ã¯
                "ä¼šè­°æƒ…å ±", "æ¥½æ›²å†ç”Ÿ", "çµŒè·¯æ¤œç´¢", "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±æ¤œç´¢", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æ¤œç´¢", "ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±", "å¤©æ°—æƒ…å ±"
                ã§ã™ã€‚
                """
        )
        chain = LLMChain(llm=llm, prompt=prompt)
        # print(chain("ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ"))

        

        st.session_state.messages.append(
            {
                "role": "user", 
                "content": prompt
            }
        )
        with st.chat_message("user", avatar = "ğŸ˜Š"):
            st.markdown(prompt)

        # with st.chat_message("assistant" , avatar = "ğŸ¤–"):
        #     message_placeholder = st.empty()
        #     full_response = ""

        #     stream = client.chat.completions.create(
        #         model=st.session_state["openai_model"],
        #         messages=[
        #             {
        #                 "role": m["role"], 
        #                 "content": m["content"]
        #             }

        #             for m in st.session_state.messages
        #         ],
        #         stream=True,
        #     )

        #     for chunk in stream:
        #         if chunk.choices[0].delta.content is not None:
        #             full_response += chunk.choices[0].delta.content
        #         message_placeholder.markdown(full_response + "â–Œ")
        #     message_placeholder.markdown(full_response)
        # st.session_state.messages.append(
        #     {
        #         "role": "assistant", 
        #         "content": full_response
        #     }
        # )
        # st.session_state.Clear = True
        with st.chat_message("assistant" , avatar = "ğŸ‘©â€ğŸ“"):
            # st_callback = StreamlitCallbackHandler(st.container())
            # cfg = RunnableConfig()
            # cfg["callbacks"] = [st_callback]
            # response = agent.invoke(user_prompt, cfg)
            response = chain(prompt)
            # response = response["output"]
            response = response["text"]
            st.write(response)
        st.session_state.messages.append(
            {
                "role": "assistant", 
                "content": response
            }
        )
        st.session_state.Clear = True

    if st.session_state.Clear:
        if st.button('clear chat history'):
            st.session_state.messages = []
            full_response = ""
            st.session_state.Clear = False 
            st.rerun()

if __name__ == "__main__":
    if not hasattr(st.session_state, "openai_api_key"):
        try:
            st.session_state.openai_api_key = os.environ["OPENAI_API_KEY"]
        except:
            st.session_state.openai_api_key = ""
    with st.sidebar:
        openai_api_key = st.text_input("OpenAI API Key", type="password")
        if not openai_api_key == "":
            st.session_state.openai_api_key = openai_api_key
        st.write("if you are running the app locally,  \nthere is no need to enter the key  \nif it is already set as an environment variable.")
    chat()